{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# HW5 \u2013 Final Stacking-Only Ensemble (S2: XGBoost Meta-Model)\n", "\n", "This notebook implements a **full end-to-end pipeline** for the wafer map classification homework,\n", "using a **Stacking ensemble** as the final and only model (no voting):\n", "\n", "1. Load structured `.npy` files:\n", "   - `wafermap_train.npy` (with fields including `waferMap`, `failureType`, `dieSize`, ...)\n", "   - `wafermap_test.npy` (with fields including `waferMap`, `dieSize`, ...)\n", "2. Extract:\n", "   - Wafer maps as images (variable shapes)\n", "   - Failure types as labels (train only)\n", "   - `dieSize` as a scalar feature\n", "3. Resize each wafer map to **64\u00d764** and compute geometric features + `dieSize` (tabular features).\n", "4. Train three base models:\n", "   - **CNN-only classifier** (ResNet18, grayscale input, 30 epochs with early stopping)\n", "   - **ExtraTreesClassifier** (tabular-only)\n", "   - **GradientBoostingClassifier** (tabular-only)\n", "5. On a validation split:\n", "   - Get `predict_proba` from each base model.\n", "   - Concatenate probabilities \u2192 stacking features.\n", "   - Train a **Stacking meta-model** using **XGBoost** (or LogisticRegression fallback).\n", "6. On the full training set:\n", "   - Retrain ExtraTrees & GradientBoost.\n", "   - Get full-train & test probabilities for all base models.\n", "   - Train the final stacking meta-model on full training stacking features.\n", "7. Predict test failure types and save `scores.csv` with a single column `failureType`.\n", "\n", "> Place this notebook in the same folder as `wafermap_train.npy` and `wafermap_test.npy`."]}, {"cell_type": "code", "metadata": {}, "source": ["import os\n", "from pathlib import Path\n", "\n", "import numpy as np\n", "import pandas as pd\n", "\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import accuracy_score\n", "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n", "from sklearn.linear_model import LogisticRegression\n", "\n", "from skimage.transform import resize\n", "from skimage.measure import label, regionprops, perimeter\n", "\n", "import torch\n", "import torch.nn as nn\n", "import torch.optim as optim\n", "from torch.utils.data import Dataset, DataLoader\n", "from torchvision import models\n", "\n", "from tqdm import tqdm\n", "\n", "# Try to import XGBoost as a stronger meta-model; fall back to LogisticRegression if unavailable.\n", "try:\n", "    from xgboost import XGBClassifier\n", "    HAS_XGB = True\n", "except ImportError:\n", "    XGBClassifier = None\n", "    HAS_XGB = False\n", "\n", "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n", "print('Using device:', device)\n", "print('Has XGBoost:', HAS_XGB)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Load structured `.npy` data"]}, {"cell_type": "code", "metadata": {}, "source": ["train_raw = np.load('wafermap_train.npy', allow_pickle=True)\n", "test_raw  = np.load('wafermap_test.npy',  allow_pickle=True)\n", "print('Train size:', len(train_raw))\n", "print('Test  size:', len(test_raw))\n", "print('Train dtype:', train_raw.dtype)\n", "print('Test  dtype:', test_raw.dtype)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Extract waferMap (list), failureType, dieSize"]}, {"cell_type": "code", "metadata": {}, "source": ["X_maps_full = [r['waferMap'] for r in train_raw]\n", "y_labels_full = [r['failureType'] for r in train_raw]\n", "die_full = np.array([r['dieSize'] for r in train_raw], float)\n", "\n", "X_maps_test = [r['waferMap'] for r in test_raw]\n", "die_test = np.array([r['dieSize'] for r in test_raw], float)\n", "\n", "print('Sample train map shape:', X_maps_full[0].shape)\n", "print('Sample test  map shape:', X_maps_test[0].shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Geometric features + dieSize (tabular features)"]}, {"cell_type": "code", "metadata": {}, "source": ["def resize_w(mp, target=(64, 64)):\n", "    \"\"\"Resize wafer map to target size using nearest-neighbor (order=0) to preserve labels.\"\"\"\n", "    return resize(mp, target, order=0, preserve_range=True, anti_aliasing=False).astype(mp.dtype)\n", "\n", "def geo_feats(mp):\n", "    \"\"\"Compute geometric features for a single wafer map.\n", "    Returns a dict with keys: area, per, maxd, mind, maj, minr, sol, ecc, yl, eyl.\n", "    \"\"\"\n", "    mp = np.array(mp)\n", "    non_die = (mp == -1)\n", "    die = ~non_die\n", "    fail = (mp > 0) & die\n", "    h, w = mp.shape\n", "    cr, cc = (h - 1) / 2, (w - 1) / 2\n", "    rad = min(h, w) / 2\n", "    total = die.sum()\n", "    if total == 0:\n", "        return dict(area=0, per=0, maxd=0, mind=0, maj=0, minr=0, sol=0, ecc=0, yl=0, eyl=0)\n", "    fc = fail.sum()\n", "    yl = fc / total\n", "    if fc == 0:\n", "        return dict(area=0, per=0, maxd=0, mind=0, maj=0, minr=0, sol=0, ecc=0, yl=float(yl), eyl=0)\n", "\n", "    coords = np.column_stack(np.nonzero(fail))\n", "    d = np.sqrt((coords[:, 0] - cr) ** 2 + (coords[:, 1] - cc) ** 2)\n", "    maxd, mind = d.max() / rad, d.min() / rad\n", "\n", "    per = perimeter(fail.astype(float)) / rad\n", "\n", "    lab = label(fail)\n", "    regs = regionprops(lab)\n", "    if len(regs) == 0:\n", "        maj = minr = sol = ecc = 0\n", "    else:\n", "        r = max(regs, key=lambda x: x.area)\n", "        maj = r.major_axis_length / (2 * rad)\n", "        minr = r.minor_axis_length / (2 * rad)\n", "        sol = r.solidity or 0\n", "        ecc = r.eccentricity or 0\n", "\n", "    eb = np.zeros_like(die, bool)\n", "    eb[0:2, :] = eb[-2:, :] = True\n", "    eb[:, 0:2] = eb[:, -2:] = True\n", "    eb &= die\n", "    td = eb.sum()\n", "    eyl = (fail & eb).sum() / td if td > 0 else 0\n", "\n", "    return dict(area=fc / total, per=per, maxd=maxd, mind=mind,\n", "                maj=maj, minr=minr, sol=sol, ecc=ecc, yl=yl, eyl=eyl)\n", "\n", "def extract_tab(maps, dies):\n", "    feats = []\n", "    for mp, dsize in zip(maps, dies):\n", "        r = geo_feats(resize_w(mp))\n", "        r['die'] = float(dsize)\n", "        feats.append(r)\n", "    return pd.DataFrame(feats)\n", "\n", "X_tab_full = extract_tab(X_maps_full, die_full)\n", "X_tab_test = extract_tab(X_maps_test, die_test)\n", "print('Tabular train shape:', X_tab_full.shape)\n", "print('Tabular test shape:', X_tab_test.shape)\n", "X_tab_full.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Label encoding + unified train/valid split (index-based)"]}, {"cell_type": "code", "metadata": {}, "source": ["le = LabelEncoder()\n", "y_enc_full = le.fit_transform(y_labels_full)\n", "num_classes = len(le.classes_)\n", "print('Classes:', list(le.classes_))\n", "\n", "indices = np.arange(len(X_maps_full))\n", "idx_tr, idx_va, y_tr, y_va = train_test_split(\n", "    indices, y_enc_full, test_size=0.2, random_state=42, stratify=y_enc_full\n", ")\n", "\n", "maps_tr = [X_maps_full[i] for i in idx_tr]\n", "maps_va = [X_maps_full[i] for i in idx_va]\n", "tab_tr = X_tab_full.iloc[idx_tr].values\n", "tab_va = X_tab_full.iloc[idx_va].values\n", "\n", "print('Train size:', len(maps_tr), 'Valid size:', len(maps_va))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. WaferDataset + DataLoaders for CNN"]}, {"cell_type": "code", "metadata": {}, "source": ["class WaferDataset(Dataset):\n", "    def __init__(self, maps, labels=None):\n", "        self.maps = maps\n", "        self.labels = labels\n", "    def __len__(self):\n", "        return len(self.maps)\n", "    def __getitem__(self, idx):\n", "        mp = resize_w(self.maps[idx]).astype('float32')\n", "        mp = (mp - mp.mean()) / (mp.std() + 1e-6)\n", "        mp = np.expand_dims(mp, 0)  # 1-channel\n", "        if self.labels is None:\n", "            return torch.tensor(mp)\n", "        else:\n", "            return torch.tensor(mp), torch.tensor(self.labels[idx], dtype=torch.long)\n", "\n", "train_ds = WaferDataset(maps_tr, y_tr)\n", "valid_ds = WaferDataset(maps_va, y_va)\n", "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n", "valid_loader = DataLoader(valid_ds, batch_size=32)\n", "print('DataLoaders ready.')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6. ResNet18 CNN-only classifier (backbone + head)"]}, {"cell_type": "code", "metadata": {}, "source": ["class ResNetBackbone(nn.Module):\n", "    def __init__(self, emb_dim=128):\n", "        super().__init__()\n", "        m = models.resnet18(weights=None)\n", "        m.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n", "        self.features = nn.Sequential(*list(m.children())[:-1])\n", "        self.fc = nn.Linear(512, emb_dim)\n", "    def forward(self, x):\n", "        x = self.features(x)\n", "        x = torch.flatten(x, 1)\n", "        x = self.fc(x)\n", "        return x\n", "\n", "class CNNClassifier(nn.Module):\n", "    def __init__(self, num_classes, emb_dim=128):\n", "        super().__init__()\n", "        self.backbone = ResNetBackbone(emb_dim)\n", "        self.head = nn.Linear(emb_dim, num_classes)\n", "    def forward(self, x, return_emb=False):\n", "        z = self.backbone(x)\n", "        if return_emb:\n", "            return z\n", "        return self.head(z)\n", "\n", "cnn_model = CNNClassifier(num_classes=num_classes, emb_dim=128).to(device)\n", "crit_cnn = nn.CrossEntropyLoss()\n", "opt_cnn = optim.Adam(cnn_model.parameters(), lr=1e-3)\n", "print('CNN model ready.')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7. Train CNN (30 epochs) with early stopping (patience=5)"]}, {"cell_type": "code", "metadata": {}, "source": ["def eval_cnn(model, loader):\n", "    model.eval()\n", "    correct = total = 0\n", "    with torch.no_grad():\n", "        for xb, yb in loader:\n", "            xb, yb = xb.to(device), yb.to(device)\n", "            logits = model(xb)\n", "            pred = logits.argmax(1)\n", "            correct += (pred == yb).sum().item()\n", "            total += len(yb)\n", "    return correct / total\n", "\n", "EPOCHS_CNN = 30\n", "PATIENCE = 5\n", "best_val_acc = 0.0\n", "wait = 0\n", "\n", "for epoch in range(EPOCHS_CNN):\n", "    cnn_model.train()\n", "    running_loss = 0.0\n", "    for xb, yb in tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS_CNN}'):\n", "        xb, yb = xb.to(device), yb.to(device)\n", "        opt_cnn.zero_grad()\n", "        logits = cnn_model(xb)\n", "        loss = crit_cnn(logits, yb)\n", "        loss.backward()\n", "        opt_cnn.step()\n", "        running_loss += loss.item() * len(yb)\n", "    train_loss = running_loss / len(train_ds)\n", "    val_acc = eval_cnn(cnn_model, valid_loader)\n", "    print(f'Epoch {epoch+1}/{EPOCHS_CNN} - train loss: {train_loss:.4f}, val acc: {val_acc:.4f}')\n", "\n", "    if val_acc > best_val_acc:\n", "        best_val_acc = val_acc\n", "        wait = 0\n", "        torch.save(cnn_model.state_dict(), 'best_cnn.pth')\n", "        print('  Saved new best CNN model.')\n", "    else:\n", "        wait += 1\n", "        if wait >= PATIENCE:\n", "            print('Early stopping triggered.')\n", "            break\n", "\n", "# Load best weights\n", "cnn_model.load_state_dict(torch.load('best_cnn.pth', map_location=device))\n", "print('Loaded best CNN weights with val acc =', best_val_acc)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8. Train ExtraTrees & GradientBoosting (tabular-only base models)"]}, {"cell_type": "code", "metadata": {}, "source": ["et_model = ExtraTreesClassifier(\n", "    n_estimators=800,\n", "    max_depth=None,\n", "    min_samples_split=2,\n", "    min_samples_leaf=1,\n", "    max_features='sqrt',\n", "    class_weight='balanced_subsample',\n", "    n_jobs=-1,\n", "    random_state=42,\n", ")\n", "\n", "gb_model = GradientBoostingClassifier(\n", "    n_estimators=300,\n", "    learning_rate=0.05,\n", "    max_depth=3,\n", "    random_state=42,\n", ")\n", "\n", "et_model.fit(tab_tr, y_tr)\n", "gb_model.fit(tab_tr, y_tr)\n", "\n", "et_va_probs = et_model.predict_proba(tab_va)\n", "gb_va_probs = gb_model.predict_proba(tab_va)\n", "et_va_pred = et_va_probs.argmax(1)\n", "gb_va_pred = gb_va_probs.argmax(1)\n", "et_va_acc = accuracy_score(y_va, et_va_pred)\n", "gb_va_acc = accuracy_score(y_va, gb_va_pred)\n", "print('ExtraTrees val acc:', et_va_acc)\n", "print('GradBoost  val acc:', gb_va_acc)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 9. CNN validation probabilities (base model 1)"]}, {"cell_type": "code", "metadata": {}, "source": ["def get_cnn_probs(model, loader):\n", "    model.eval()\n", "    all_probs = []\n", "    all_labels = []\n", "    with torch.no_grad():\n", "        for xb, yb in loader:\n", "            xb = xb.to(device)\n", "            logits = model(xb)\n", "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n", "            all_probs.append(probs)\n", "            all_labels.append(yb.numpy())\n", "    return np.vstack(all_probs), np.concatenate(all_labels)\n", "\n", "cnn_va_probs, cnn_va_labels = get_cnn_probs(cnn_model, valid_loader)\n", "cnn_va_pred = cnn_va_probs.argmax(1)\n", "cnn_va_acc = accuracy_score(y_va, cnn_va_pred)\n", "print('CNN-only val acc:', cnn_va_acc)\n", "assert np.array_equal(cnn_va_labels, y_va), 'Label mismatch in CNN valid loader'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 10. Stacking meta-model on validation (S2: XGBoost if available, otherwise LogisticRegression)"]}, {"cell_type": "code", "metadata": {}, "source": ["# Stacking features on validation\n", "stack_va_feats = np.concatenate([cnn_va_probs, et_va_probs, gb_va_probs], axis=1)\n", "\n", "if HAS_XGB:\n", "    stack_meta_val = XGBClassifier(\n", "        n_estimators=400,\n", "        learning_rate=0.05,\n", "        max_depth=4,\n", "        subsample=0.9,\n", "        colsample_bytree=0.8,\n", "        objective='multi:softprob',\n", "        num_class=num_classes,\n", "        tree_method='hist',\n", "        random_state=42,\n", "    )\n", "else:\n", "    stack_meta_val = LogisticRegression(max_iter=1000, multi_class='auto')\n", "\n", "stack_meta_val.fit(stack_va_feats, y_va)\n", "stack_va_pred = stack_meta_val.predict(stack_va_feats)\n", "stack_va_acc = accuracy_score(y_va, stack_va_pred)\n", "print('Stacking (meta-model) val acc =', stack_va_acc)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 11. Retrain tabular base models on full training data & get full/test probabilities"]}, {"cell_type": "code", "metadata": {}, "source": ["et_full = ExtraTreesClassifier(\n", "    n_estimators=800,\n", "    max_depth=None,\n", "    min_samples_split=2,\n", "    min_samples_leaf=1,\n", "    max_features='sqrt',\n", "    class_weight='balanced_subsample',\n", "    n_jobs=-1,\n", "    random_state=42,\n", ")\n", "\n", "gb_full = GradientBoostingClassifier(\n", "    n_estimators=300,\n", "    learning_rate=0.05,\n", "    max_depth=3,\n", "    random_state=42,\n", ")\n", "\n", "et_full.fit(X_tab_full.values, y_enc_full)\n", "gb_full.fit(X_tab_full.values, y_enc_full)\n", "\n", "et_full_probs = et_full.predict_proba(X_tab_full.values)\n", "gb_full_probs = gb_full.predict_proba(X_tab_full.values)\n", "et_test_probs = et_full.predict_proba(X_tab_test.values)\n", "gb_test_probs = gb_full.predict_proba(X_tab_test.values)\n", "print('Full tabular models retrained.')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 12. CNN full-training and test probabilities"]}, {"cell_type": "code", "metadata": {}, "source": ["full_train_ds = WaferDataset(X_maps_full, y_enc_full)\n", "full_train_loader = DataLoader(full_train_ds, batch_size=32, shuffle=False)\n", "test_ds = WaferDataset(X_maps_test, None)\n", "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n", "\n", "def get_cnn_probs_infer(model, loader):\n", "    model.eval()\n", "    all_probs = []\n", "    with torch.no_grad():\n", "        for batch in loader:\n", "            if isinstance(batch, (list, tuple)):\n", "                xb = batch[0]\n", "            else:\n", "                xb = batch\n", "            xb = xb.to(device)\n", "            logits = model(xb)\n", "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n", "            all_probs.append(probs)\n", "    return np.vstack(all_probs)\n", "\n", "cnn_full_probs = get_cnn_probs_infer(cnn_model, full_train_loader)\n", "cnn_test_probs = get_cnn_probs_infer(cnn_model, test_loader)\n", "print('CNN full & test probabilities ready.')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 13. Train final stacking meta-model on full training & predict test (Stacking only)"]}, {"cell_type": "code", "metadata": {}, "source": ["# Full training stacking features\n", "stack_full_feats = np.concatenate([\n", "    cnn_full_probs,\n", "    et_full_probs,\n", "    gb_full_probs,\n", "], axis=1)\n", "\n", "if HAS_XGB:\n", "    stack_meta = XGBClassifier(\n", "        n_estimators=600,\n", "        learning_rate=0.05,\n", "        max_depth=4,\n", "        subsample=0.9,\n", "        colsample_bytree=0.8,\n", "        objective='multi:softprob',\n", "        num_class=num_classes,\n", "        tree_method='hist',\n", "        random_state=42,\n", "    )\n", "else:\n", "    stack_meta = LogisticRegression(max_iter=2000, multi_class='auto')\n", "\n", "stack_meta.fit(stack_full_feats, y_enc_full)\n", "print('Final stacking meta-model trained.')\n", "\n", "# Test stacking features\n", "stack_test_feats = np.concatenate([\n", "    cnn_test_probs,\n", "    et_test_probs,\n", "    gb_test_probs,\n", "], axis=1)\n", "\n", "final_test_enc = stack_meta.predict(stack_test_feats)\n", "final_test_labels = le.inverse_transform(final_test_enc)\n", "\n", "out_df = pd.DataFrame({'failureType': final_test_labels})\n", "out_df.to_csv('scores.csv', index=False)\n", "print('scores.csv saved.')\n", "out_df.head()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2}